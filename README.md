
<body>

  <h1>ğŸ™ï¸ Voice Mood Detection</h1>
  <p>A machine learning-based project that detects the <strong>emotional mood</strong> of a person based on their voice input. The system processes voice recordings and classifies them into moods such as <em>happy</em>, <em>sad</em>, <em>angry</em>, <em>neutral</em>, etc.</p>

  <h2>ğŸš€ Features</h2>
  <ul>
    <li>ğŸ§ Accepts voice input (WAV/MP3)</li>
    <li>ğŸ”Š Extracts relevant audio features like MFCCs, Chroma, Spectral Contrast</li>
    <li>ğŸ§  Uses trained ML models to predict emotion</li>
    <li>ğŸ“Š Displays predicted mood with probability score</li>
    <li>ğŸ’¡ Streamlit web interface for real-time usage</li>
  </ul>

  <h2>ğŸ› ï¸ Tech Stack</h2>
  <ul>
    <li><strong>Frontend:</strong> Streamlit</li>
    <li><strong>Backend:</strong> Python, Librosa, NumPy, Pandas</li>
    <li><strong>Model:</strong> Logistic Regression / Random Forest / SVM</li>
    <li><strong>Libraries:</strong> scikit-learn, librosa, matplotlib, seaborn</li>
  </ul>

  <h2>ğŸ“ Project Structure</h2>
  <pre><code>
Voice-Mood-Detection/
â”œâ”€â”€ VoiceMood-Detection.py                # Streamlit web app
â”œâ”€â”€ model.pkl             # Trained ML model
â”œâ”€â”€ Encoder.pkl             # Trained ML model
â”œâ”€â”€ requirements.txt      # Dependencies
â”œâ”€â”€ speech-emotion-recognition.ipynb     # Main ipynb
â””â”€â”€ README.md             # Project documentation
  </code></pre>

  <h2>ğŸ“¦ Installation</h2>
  <p><strong>1. Clone the repo</strong></p>
  <pre><code>git clone https://github.com/Khaleelsk/Voice-Mood-Detection.git
cd Voice-Mood-Detection</code></pre>

  <p><strong>2. Install dependencies</strong></p>
  <pre><code>pip install -r requirements.txt</code></pre>

  <p><strong>3. Run the app</strong></p>
  <pre><code>streamlit run VoiceMood-Detection.py</code></pre>

  <h2>ğŸ¯ How It Works</h2>
  <ol>
    <li>Upload a <code>.wav</code> or <code>.mp3</code> voice file.</li>
    <li>The system extracts features like:
      <ul>
        <li>MFCC (Mel-frequency cepstral coefficients)</li>
        <li>Chroma</li>
        <li>Zero Crossing Rate</li>
      </ul>
    </li>
    <li>Features are passed to the trained classifier.</li>
    <li>The app predicts the emotion with a confidence score.</li>
  </ol>

  <h2>ğŸ“Š Dataset</h2>
  <p>Trained using the <strong>RAVDESS</strong> (Ryerson Audio-Visual Database of Emotional Speech and Song) dataset containing labeled audio samples.</p>

  <h2>ğŸ§  Model Accuracy</h2>
  <p>Achieved approximately <strong>85% accuracy</strong> on test data. You can improve it by trying different models and tuning hyperparameters.</p>

  <h2>ğŸ“Œ Future Improvements</h2>
  <ul>
    <li>Real-time voice recording via microphone</li>
    <li>Support for multilingual emotion detection</li>
    <li>Deep learning model integration (LSTM/CNN)</li>
  </ul>

  <h2>ğŸ¤ Contributions</h2>
  <p>Feel free to open issues or pull requests to improve the project! UI tweaks, model upgrades, or bug fixes are all welcome.</p>

  <h2>ğŸ“ƒ License</h2>
  <p>This project is licensed under the <strong>MIT License</strong>.</p>

</body>
</html>
